{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msGqrVs7-u4u"
      },
      "source": [
        "## Coding assignment: Graph Neural Networks (GNN)\n",
        "\n",
        "Graph structures are ubiquitous in various domains, from social networks to molecular interactions. Understanding these complex relationships requires advanced analytical tools, and Graph Neural Networks (GNNs) provide a powerful framework for extracting meaningful insights from graph-structured data.\n",
        "\n",
        "In this assignment, you will:\n",
        "\n",
        "- Gain a foundational understanding of GNNs and their underlying principles,\n",
        "- Explore their applications in graph analysis, and\n",
        "- Implement GNNs using state-of-the-art deep learning frameworks.\n",
        "-\n",
        "By the end of this assignment, you will have acquired both theoretical knowledge and hands-on experience in applying GNNs to real-world graph data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfqyBI4Z-u4v"
      },
      "source": [
        "## Environment Setup\n",
        "For a seamless execution of this notebook, ensure your Python environment is properly set up. Here's what you'll need:\n",
        "\n",
        "Python Version: We recommend using Python 3.8 or higher.\n",
        "\n",
        "Required Packages: Install the following libraries to delve into GNNs:\n",
        "\n",
        "```\n",
        "torch\n",
        "torch_geometric\n",
        "torch_scatter\n",
        "torch_sparse\n",
        "torchmetrics\n",
        "networkx\n",
        "numpy\n",
        "jupyter\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD6dUEAc-u4w",
        "outputId": "9be5fd85-1340-4b70-94b6-20a77b2af5ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.5.1+cu124\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_TG8aPC-u4w"
      },
      "source": [
        "In this assignment, we will utilize the **CLUSTER** dataset from the GNNBenchmarkDataset, as introduced in the paper Benchmarking Graph Neural Networks. This dataset is a subset of the Stochastic Block Model (SBM) datasets, which focus on node-level graph pattern recognition tasks, originally explored by Scarselli et al. (2009). Specifically, it addresses the following tasks:\n",
        "- Graph Pattern Recognition (PATTERN)\n",
        "- Semi-supervised Graph Clustering (CLUSTER)\n",
        "The Stochastic Block Model (SBM), as described by Abbe (2017), serves as the foundation of these datasets. SBM is widely used for modeling community structures in social networks, where intra- and inter-community connections are probabilistically controlled. In particular:\n",
        "\n",
        "- Two nodes within the same community are connected with probability p.\n",
        "- Nodes belonging to different communities are connected with probability q, which acts as a noise parameter.\n",
        "\n",
        "The CLUSTER dataset has the following properties:\n",
        "\n",
        "- Each node is represented by a 7-dimensional feature vector.\n",
        "- The dataset contains 6 distinct node classes.\n",
        "- The primary learning objective is multi-class classification at the node level.\n",
        "\n",
        "This dataset provides a challenging yet insightful benchmark for evaluating the performance of Graph Neural Networks in community detection and clustering tasks. We will start by loading the dataset and take a look at the graphs in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzcVU9Ej-u4w",
        "outputId": "5162ddd1-8976-456a-a72f-299e0aac8551"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://data.pyg.org/datasets/benchmarking-gnns/CLUSTER_v2.zip\n",
            "Extracting data/CLUSTER/raw/CLUSTER_v2.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training dataset size: 10000\n",
            "Validation dataset size: 1000\n",
            "Graph 0:\n",
            "  - Number of nodes: 117\n",
            "  - Number of edges: 4104\n",
            "  - Node features shape: torch.Size([117, 7])\n",
            "  - Edge index shape: torch.Size([2, 4104])\n",
            "  - Labels shape: torch.Size([117])\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.datasets import GNNBenchmarkDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Cluster dataset\n",
        "data_root = './data'\n",
        "dataset_train = GNNBenchmarkDataset(root=data_root, name='CLUSTER', split='train')\n",
        "dataset_val = GNNBenchmarkDataset(root=data_root, name='CLUSTER', split='val')\n",
        "\n",
        "# Print dataset statistics\n",
        "print(f'Training dataset size: {len(dataset_train)}')\n",
        "print(f'Validation dataset size: {len(dataset_val)}')\n",
        "\n",
        "# Print a sample graph's details\n",
        "def print_graph_info(graph, index):\n",
        "    print(f'Graph {index}:')\n",
        "    print(f'  - Number of nodes: {graph.num_nodes}')\n",
        "    print(f'  - Number of edges: {graph.num_edges}')\n",
        "    print(f'  - Node features shape: {graph.x.shape}')\n",
        "    print(f'  - Edge index shape: {graph.edge_index.shape}')\n",
        "    print(f'  - Labels shape: {graph.y.shape}')\n",
        "    print('-' * 40)\n",
        "\n",
        "# Display information about the first few graphs\n",
        "for i in range(min(1, len(dataset_train))):\n",
        "    print_graph_info(dataset_train[i], i)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ9KAMcl-u4x"
      },
      "source": [
        "Then we use torch_geometric's data loader class to wrap the dataset to batches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWxwNC0X-u4x",
        "outputId": "c1839fcd-6f9f-42db-d8b1-1928a0ffdba2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batched graph:\n",
            "  - Batch size: 32\n",
            "  - Total nodes in batch: 3733\n",
            "  - Total edges in batch: 138014\n",
            "  - Labels shape: torch.Size([3733])\n"
          ]
        }
      ],
      "source": [
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Display batched graph information\n",
        "for batch in dataloader_train:\n",
        "    print('Batched graph:')\n",
        "    print(f'  - Batch size: {batch_size}')\n",
        "    print(f'  - Total nodes in batch: {batch.x.shape[0]}')\n",
        "    print(f'  - Total edges in batch: {batch.edge_index.shape[1]}')\n",
        "    print(f'  - Labels shape: {batch.y.shape}')\n",
        "    break  # Only show one batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omHBOajy-u4x"
      },
      "source": [
        "## Task 1: MLP for node classification\n",
        "\n",
        "To start with, we will build an MLP model as a baseline. The MLP should directly take the node features as input and output the predictions for each class. In this task, you need to complete the MLP model and the training function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMju7PmR-u4x"
      },
      "outputs": [],
      "source": [
        "class MLPNodeClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        #####\n",
        "        super(MLPNodeClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        #####\n",
        "\n",
        "    def forward(self, x):\n",
        "        #####\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "        #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7TTiYmk-u4x"
      },
      "source": [
        "The training and evaluation functions for the MLP model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7cGhOmf-u4x"
      },
      "outputs": [],
      "source": [
        "# Evaluation function\n",
        "def evaluate(model, dataloader, device='cuda:0'):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            #####\n",
        "            inputs = batch.x\n",
        "            labels = batch.y\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            #####\n",
        "    # return the accuracy\n",
        "    return correct / total\n",
        "\n",
        "# Training function\n",
        "def train(model, dataloader_train, dataloader_val, epochs=50, lr=0.001, patience=5, device='cuda:0'):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    best_val_acc = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0 # number of correct predicted nodes\n",
        "        total = 0 # number of nodes\n",
        "\n",
        "        for batch in dataloader_train:\n",
        "            #####\n",
        "            inputs = batch.x\n",
        "            labels = batch.y  # Assuming dataloader returns (inputs, labels)\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            #####\n",
        "\n",
        "        train_acc = correct / total\n",
        "        val_acc = evaluate(model, dataloader_val, device)\n",
        "        print(f'Epoch {epoch+1}: Loss={total_loss:.4f}, Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}')\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            best_ckpt = model.state_dict()\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print('Early stopping triggered')\n",
        "            break\n",
        "    return best_ckpt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zuj2UqZi-u4x"
      },
      "source": [
        "We will train the MLP for the node classification task. There are several hyperparameters that might need your attention, including the batch size, hidden dimension of the MLP, number of epochs, learning rate, and early stop patience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSK46-eb-u4x",
        "outputId": "84d22f8a-c188-456e-b981-a54d4f85c8fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLPNodeClassifier(\n",
            "  (fc1): Linear(in_features=7, out_features=1280, bias=True)\n",
            "  (fc2): Linear(in_features=1280, out_features=6, bias=True)\n",
            ")\n",
            "Epoch 1: Loss=546.3391, Train Acc=0.2066, Val Acc=0.2133\n",
            "Epoch 2: Loss=534.6440, Train Acc=0.2086, Val Acc=0.2105\n",
            "Epoch 3: Loss=532.9793, Train Acc=0.2093, Val Acc=0.2105\n",
            "Epoch 4: Loss=532.5838, Train Acc=0.2101, Val Acc=0.2105\n",
            "Early stopping triggered\n"
          ]
        }
      ],
      "source": [
        "# Initialize model\n",
        "input_dim = dataset_train.num_node_features\n",
        "hidden_dim = 1280\n",
        "output_dim = dataset_train.num_classes\n",
        "\n",
        "model = MLPNodeClassifier(input_dim, hidden_dim, output_dim)\n",
        "print(model)\n",
        "\n",
        "# Train the model\n",
        "best_ckpt = train(model, dataloader_train, dataloader_val, epochs=10, lr=1e-4, patience=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx_bOyRO-u4y"
      },
      "source": [
        "After the training, we will make predictions on the test set and save the prediction results. The saved predictions will be used for grading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYl8jLUp-u4y"
      },
      "outputs": [],
      "source": [
        "dataset_test = GNNBenchmarkDataset(root=data_root, name='CLUSTER', split='test')\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Test set predictions and save results\n",
        "def predict(model, dataloader, filename='predictions.txt', device='cuda:0'):\n",
        "    model.eval()\n",
        "    predictions = [] # a list of predicted labels\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            #####\n",
        "            inputs = batch.x\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            #####\n",
        "\n",
        "    return predictions\n",
        "\n",
        "model.load_state_dict(best_ckpt)\n",
        "predictions = predict(model, dataloader_test)\n",
        "np.savetxt('predictions_mlp_cluster.txt', predictions, fmt='%d')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EyO-uwB-u4y"
      },
      "source": [
        "## Task 2: GCN for node classification\n",
        "\n",
        "Next we will leverage the graph convolutional layers to construct a GNN model for the node classification task. You can check pyg's [documentation](https://pytorch-geometric.readthedocs.io/en/latest/) to learn the usage of the `GCNConv` module and use it to build a GNN model. In the following, you need to complete the `GNNNodeClassifier` class as well as the training function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZfvkKmN-u4y"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GCNConv\n",
        "import torch.nn.functional as F\n",
        "# Define a GNN model using GCNConv\n",
        "class GNNNodeClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_gcn_layers=10):\n",
        "        #####\n",
        "        super(GNNNodeClassifier, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, output_dim)\n",
        "        #####\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        #####\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "        #####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBo-NfzS-u4y"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "# Training function\n",
        "def train(model, dataloader_train, dataloader_val, epochs=50, lr=0.001, patience=5, device='cuda:0'):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)\n",
        "    best_val_acc = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch in dataloader_train:\n",
        "            #####\n",
        "            inputs = batch.x\n",
        "            labels = batch.y\n",
        "            edge_index = batch.edge_index\n",
        "            inputs, labels, edge_index = inputs.to(device), labels.to(device), edge_index.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs,edge_index)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            #####\n",
        "\n",
        "        train_acc = correct / total\n",
        "        val_acc = evaluate(model, dataloader_val, device)\n",
        "        scheduler.step(val_acc)\n",
        "        print(f'Epoch {epoch+1}: Loss={total_loss:.4f}, Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}')\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print('Early stopping triggered')\n",
        "            break\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, dataloader, device='cuda:0'):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            #####\n",
        "            inputs = batch.x\n",
        "            labels = batch.y\n",
        "            edge_index = batch.edge_index\n",
        "            inputs, labels, edge_index = inputs.to(device), labels.to(device), edge_index.to(device)\n",
        "\n",
        "            outputs = model(inputs,edge_index)\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            #####\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy5lgJc8-u4y"
      },
      "source": [
        "Then we can train the GCN model for the node classification. You should carefully choose the hyperparameters: batch size, hidden dimension, number of GCN layers, number of epochs, learning rate, and early stop patience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WU2KOlT3-u4y",
        "outputId": "c0670faa-b663-4a45-a0a6-d915a1e45593"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GNNNodeClassifier(\n",
            "  (conv1): GCNConv(7, 512)\n",
            "  (conv2): GCNConv(512, 6)\n",
            ")\n",
            "Number of trainable params: 7174\n",
            "Epoch 1: Loss=559.0032, Train Acc=0.2157, Val Acc=0.3102\n",
            "Epoch 2: Loss=548.5375, Train Acc=0.3352, Val Acc=0.3697\n",
            "Epoch 3: Loss=526.3581, Train Acc=0.4155, Val Acc=0.4252\n",
            "Epoch 4: Loss=501.2557, Train Acc=0.4417, Val Acc=0.4523\n",
            "Epoch 5: Loss=481.8448, Train Acc=0.4533, Val Acc=0.4517\n",
            "Epoch 6: Loss=468.5315, Train Acc=0.4580, Val Acc=0.4586\n",
            "Epoch 7: Loss=459.4337, Train Acc=0.4599, Val Acc=0.4583\n",
            "Epoch 8: Loss=453.3189, Train Acc=0.4609, Val Acc=0.4606\n",
            "Epoch 9: Loss=448.8634, Train Acc=0.4626, Val Acc=0.4613\n",
            "Epoch 10: Loss=445.7194, Train Acc=0.4631, Val Acc=0.4603\n",
            "Epoch 11: Loss=443.5800, Train Acc=0.4635, Val Acc=0.4617\n",
            "Epoch 12: Loss=441.9017, Train Acc=0.4641, Val Acc=0.4637\n",
            "Epoch 13: Loss=440.7250, Train Acc=0.4644, Val Acc=0.4649\n",
            "Epoch 14: Loss=439.8229, Train Acc=0.4648, Val Acc=0.4637\n",
            "Epoch 15: Loss=439.0982, Train Acc=0.4655, Val Acc=0.4640\n",
            "Epoch 16: Loss=438.5346, Train Acc=0.4654, Val Acc=0.4648\n",
            "Epoch 17: Loss=438.1958, Train Acc=0.4657, Val Acc=0.4657\n",
            "Epoch 18: Loss=437.7422, Train Acc=0.4661, Val Acc=0.4645\n",
            "Epoch 19: Loss=437.5939, Train Acc=0.4660, Val Acc=0.4648\n",
            "Epoch 20: Loss=437.2295, Train Acc=0.4667, Val Acc=0.4653\n",
            "Epoch 21: Loss=436.9719, Train Acc=0.4667, Val Acc=0.4665\n",
            "Epoch 22: Loss=436.8409, Train Acc=0.4665, Val Acc=0.4661\n",
            "Epoch 23: Loss=436.5555, Train Acc=0.4671, Val Acc=0.4665\n",
            "Epoch 24: Loss=436.4602, Train Acc=0.4672, Val Acc=0.4650\n",
            "Epoch 25: Loss=436.3689, Train Acc=0.4670, Val Acc=0.4670\n",
            "Epoch 26: Loss=436.1260, Train Acc=0.4673, Val Acc=0.4649\n",
            "Epoch 27: Loss=436.0860, Train Acc=0.4675, Val Acc=0.4629\n",
            "Epoch 28: Loss=436.0449, Train Acc=0.4673, Val Acc=0.4648\n",
            "Epoch 29: Loss=435.8739, Train Acc=0.4675, Val Acc=0.4660\n",
            "Epoch 30: Loss=435.7532, Train Acc=0.4679, Val Acc=0.4653\n",
            "Early stopping triggered\n"
          ]
        }
      ],
      "source": [
        "# Initialize model\n",
        "input_dim = dataset_train.num_node_features\n",
        "hidden_dim = 512\n",
        "output_dim = dataset_train.num_classes\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model = GNNNodeClassifier(input_dim, hidden_dim, output_dim, num_gcn_layers=5)\n",
        "print(model)\n",
        "print(f'Number of trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad)}')\n",
        "model.to(device)\n",
        "\n",
        "# Train the model\n",
        "train(model, dataloader_train, dataloader_val, epochs=30, lr=1e-3, patience=5, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtJG6rma-u4y"
      },
      "source": [
        "After training, we can make predictions on the test set and save the results. The results will be used for the grading of the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOS65DqO-u4y"
      },
      "outputs": [],
      "source": [
        "# Test set predictions and save results\n",
        "def predict(model, dataloader, filename='gnn_predictions.txt', device='cuda:0'):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            #####\n",
        "            inputs = batch.x\n",
        "            labels = batch.y\n",
        "            edge_index = batch.edge_index\n",
        "            inputs, labels, edge_index = inputs.to(device), labels.to(device), edge_index.to(device)\n",
        "\n",
        "            outputs = model(inputs,edge_index)\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            #####\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "predictions = predict(model, dataloader_test)\n",
        "np.savetxt('predictions_gcn_cluster.txt', predictions, fmt='%d')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsgOUUCb-u4z"
      },
      "source": [
        "## Graph classification task\n",
        "\n",
        "In this section, we explore graph classification using Graph Neural Networks (GNNs). Unlike node classification, which focuses on predicting labels for individual nodes within a graph, graph classification aims to assign labels to entire graphs based on their structural and feature-based attributes. The primary challenge lies in effectively embedding entire graphs into a feature space where they become linearly separable for classification tasks.\n",
        "\n",
        "One notable application of graph classification is the representation of image data as graphs, an approach demonstrated in super-pixel datasets. These datasets provide a novel way to transform traditional image classification tasks into graph learning problems. Prominent image datasets such as MNIST and CIFAR10 have been adapted into graph structures using this methodology. The motivation for utilizing these datasets is twofold:\n",
        "\n",
        "- Benchmarking and Sanity-Checking – These datasets serve as standard benchmarks for evaluating the performance of GNN architectures. Most GNN models are expected to achieve near-perfect accuracy on MNIST and competitive performance on CIFAR10.\n",
        "- Extending Image-Based Learning to Graphs – Super-pixel representations offer valuable insights into how conventional image datasets can be leveraged for graph-based learning and analysis.\n",
        "\n",
        "### CIFAR10 Super-Pixel Dataset\n",
        "In this assignment, we will work with the CIFAR10 super-pixel dataset for a graph classification task. The CIFAR10 images are transformed into graphs using super-pixel segmentation, where each super-pixel represents a small, homogeneous region of the image. This transformation is performed using the Simple Linear Iterative Clustering (SLIC) algorithm, introduced by Achanta et al. (2012).\n",
        "\n",
        "By leveraging super-pixel representations, we can analyze the effectiveness of GNNs in graph classification while drawing connections between traditional computer vision tasks and graph-based learning techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUpEQMJ1-u4z",
        "outputId": "03b0445c-892f-44cc-adca-2d71ebea6e10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training dataset size: 45000\n",
            "Validation dataset size: 5000\n",
            "Graph 0:\n",
            "  - Number of nodes: 110\n",
            "  - Number of edges: 880\n",
            "  - Node features shape: torch.Size([110, 3])\n",
            "  - Edge index shape: torch.Size([2, 880])\n",
            "  - Labels shape: torch.Size([1])\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool, global_max_pool, GraphNorm\n",
        "\n",
        "# Load the CIFAR10 dataset\n",
        "data_root = './data/GNNBenchmark'\n",
        "dataset_train = GNNBenchmarkDataset(root=data_root, name='CIFAR10', split='train')\n",
        "dataset_val = GNNBenchmarkDataset(root=data_root, name='CIFAR10', split='val')\n",
        "\n",
        "print(f'Training dataset size: {len(dataset_train)}')\n",
        "print(f'Validation dataset size: {len(dataset_val)}')\n",
        "print_graph_info(dataset_train[0], 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XF0xrqM-u4z"
      },
      "source": [
        "## Task 3: GCN for graph classification\n",
        "\n",
        "In this task, you need to build a GNN model using the `GCNConv` module for the graph classification task. Note that in order to do graph classification, you need to get the graph embedding by pooling the node embeddings. You can refer to pyg's [documentation](https://pytorch-geometric.readthedocs.io/en/latest/) for different pooling functions (e.g., mean pooling, max pooling, sum pooling)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8Epk8eO-u4z"
      },
      "outputs": [],
      "source": [
        "# Define a GCN model for graph classification\n",
        "class GCNGraphClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_gcn_layers=2):\n",
        "        #####\n",
        "        super(GCNGraphClassifier, self).__init__()\n",
        "\n",
        "        #Define GCN layers\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.norms = nn.ModuleList()\n",
        "        self.convs.append(GCNConv(input_dim, hidden_dim))  #First layer\n",
        "        self.norms.append(GraphNorm(hidden_dim))\n",
        "\n",
        "        for _ in range(num_gcn_layers - 2):\n",
        "            self.convs.append(GCNConv(hidden_dim, hidden_dim))  #Hidden layers\n",
        "            self.norms.append(GraphNorm(hidden_dim))\n",
        "        self.convs.append(GCNConv(hidden_dim, output_dim))  #Final GCN layer\n",
        "\n",
        "        #Global pooling layer to obtain graph-level representation\n",
        "        self.pool = global_mean_pool\n",
        "\n",
        "        #Fully connected layer for final classification\n",
        "        self.fc = nn.Linear(output_dim, 10)\n",
        "        #####\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        #####\n",
        "        for i in range(len(self.convs) - 1):  # Apply GraphNorm to hidden layers only\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            x = self.norms[i](x)  # Correct usage of GraphNorm\n",
        "            x = F.relu(x)\n",
        "\n",
        "        x = self.convs[-1](x, edge_index)\n",
        "        # Aggregate node embeddings into a graph representation\n",
        "        x = self.pool(x, batch)\n",
        "\n",
        "        # Fully connected classification layer\n",
        "        return self.fc(x)\n",
        "        #####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMxwAREg-u4z"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train(model, dataloader_train, dataloader_val, epochs=50, lr=0.001, patience=5, device='cuda:0'):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    best_val_acc = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch in dataloader_train:\n",
        "            #####\n",
        "            batch = batch.to(device)  # Move batch to device (GPU or CPU)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch.x, batch.edge_index, batch.batch)  # Forward pass\n",
        "            loss = criterion(outputs, batch.y)  # Compute loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, dim=1)  # Get predicted class indices\n",
        "            correct += (predicted == batch.y).sum().item()\n",
        "            total += batch.y.size(0)\n",
        "            #####\n",
        "\n",
        "        train_acc = correct / total\n",
        "        val_acc = evaluate(model, dataloader_val, device)\n",
        "        print(f'Epoch {epoch+1}: Loss={total_loss:.4f}, Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}')\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print('Early stopping triggered')\n",
        "            break\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, dataloader, device='cuda:0'):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            #####\n",
        "            batch = batch.to(device)  # Move batch to device\n",
        "            outputs = model(batch.x, batch.edge_index, batch.batch)  # Forward pass\n",
        "            _, predicted = torch.max(outputs, dim=1)  # Get predicted class indices\n",
        "            correct += (predicted == batch.y).sum().item()\n",
        "            total += batch.y.size(0)\n",
        "            #####\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWDwa5Jn-u4z"
      },
      "source": [
        "Train the GCN model for graph classification:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2WZidcx-u4z",
        "outputId": "74a214f6-5c59-46c3-a9e7-f7a0866a296b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GCNGraphClassifier(\n",
            "  (convs): ModuleList(\n",
            "    (0): GCNConv(3, 128)\n",
            "    (1-6): 6 x GCNConv(128, 128)\n",
            "    (7): GCNConv(128, 10)\n",
            "  )\n",
            "  (norms): ModuleList(\n",
            "    (0-6): 7 x GraphNorm(128)\n",
            "  )\n",
            "  (fc): Linear(in_features=10, out_features=10, bias=True)\n",
            ")\n",
            "Number of trainable params: 103672\n",
            "Epoch 1: Loss=2817.7209, Train Acc=0.2635, Val Acc=0.2824\n",
            "Epoch 2: Loss=2740.4002, Train Acc=0.2847, Val Acc=0.2880\n",
            "Epoch 3: Loss=2713.3685, Train Acc=0.2968, Val Acc=0.2920\n",
            "Epoch 4: Loss=2693.4344, Train Acc=0.3022, Val Acc=0.2990\n",
            "Epoch 5: Loss=2677.7845, Train Acc=0.3062, Val Acc=0.3122\n",
            "Epoch 6: Loss=2656.2311, Train Acc=0.3128, Val Acc=0.3106\n",
            "Epoch 7: Loss=2642.4032, Train Acc=0.3160, Val Acc=0.3100\n",
            "Epoch 8: Loss=2628.8232, Train Acc=0.3208, Val Acc=0.3218\n",
            "Epoch 9: Loss=2619.3878, Train Acc=0.3225, Val Acc=0.3240\n",
            "Epoch 10: Loss=2603.8775, Train Acc=0.3282, Val Acc=0.3244\n",
            "Epoch 11: Loss=2595.3688, Train Acc=0.3306, Val Acc=0.3216\n",
            "Epoch 12: Loss=2591.8770, Train Acc=0.3303, Val Acc=0.3320\n",
            "Epoch 13: Loss=2578.6475, Train Acc=0.3346, Val Acc=0.3298\n",
            "Epoch 14: Loss=2575.7012, Train Acc=0.3372, Val Acc=0.3240\n",
            "Epoch 15: Loss=2560.8983, Train Acc=0.3410, Val Acc=0.3310\n",
            "Epoch 16: Loss=2554.0423, Train Acc=0.3434, Val Acc=0.3350\n",
            "Epoch 17: Loss=2547.2316, Train Acc=0.3435, Val Acc=0.3294\n",
            "Epoch 18: Loss=2539.8314, Train Acc=0.3472, Val Acc=0.3352\n",
            "Epoch 19: Loss=2538.6500, Train Acc=0.3483, Val Acc=0.3338\n",
            "Epoch 20: Loss=2533.0350, Train Acc=0.3502, Val Acc=0.3460\n",
            "Epoch 21: Loss=2524.7469, Train Acc=0.3487, Val Acc=0.3432\n",
            "Epoch 22: Loss=2515.5327, Train Acc=0.3539, Val Acc=0.3458\n",
            "Epoch 23: Loss=2515.5448, Train Acc=0.3535, Val Acc=0.3448\n",
            "Epoch 24: Loss=2510.5080, Train Acc=0.3557, Val Acc=0.3460\n",
            "Epoch 25: Loss=2502.4427, Train Acc=0.3530, Val Acc=0.3474\n",
            "Epoch 26: Loss=2499.6827, Train Acc=0.3586, Val Acc=0.3478\n",
            "Epoch 27: Loss=2492.7920, Train Acc=0.3580, Val Acc=0.3430\n",
            "Epoch 28: Loss=2488.5464, Train Acc=0.3588, Val Acc=0.3452\n",
            "Epoch 29: Loss=2485.1111, Train Acc=0.3600, Val Acc=0.3514\n",
            "Epoch 30: Loss=2482.3796, Train Acc=0.3626, Val Acc=0.3488\n",
            "Epoch 31: Loss=2475.6110, Train Acc=0.3609, Val Acc=0.3546\n",
            "Epoch 32: Loss=2471.3731, Train Acc=0.3622, Val Acc=0.3506\n",
            "Epoch 33: Loss=2465.6187, Train Acc=0.3643, Val Acc=0.3576\n",
            "Epoch 34: Loss=2460.8276, Train Acc=0.3683, Val Acc=0.3542\n",
            "Epoch 35: Loss=2455.7865, Train Acc=0.3679, Val Acc=0.3608\n",
            "Epoch 36: Loss=2453.0703, Train Acc=0.3683, Val Acc=0.3574\n",
            "Epoch 37: Loss=2449.3549, Train Acc=0.3723, Val Acc=0.3592\n",
            "Epoch 38: Loss=2446.2077, Train Acc=0.3721, Val Acc=0.3528\n",
            "Epoch 39: Loss=2444.7704, Train Acc=0.3727, Val Acc=0.3492\n",
            "Epoch 40: Loss=2438.5899, Train Acc=0.3735, Val Acc=0.3600\n",
            "Early stopping triggered\n"
          ]
        }
      ],
      "source": [
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize models\n",
        "input_dim = dataset_train.num_node_features\n",
        "hidden_dim = 128\n",
        "output_dim = dataset_train.num_classes\n",
        "\n",
        "gcn_model = GCNGraphClassifier(input_dim, hidden_dim, output_dim, num_gcn_layers=8)\n",
        "gcn_model.to(device)\n",
        "print(gcn_model)\n",
        "print(f'Number of trainable params: {sum(p.numel() for p in gcn_model.parameters() if p.requires_grad)}')\n",
        "\n",
        "# Train the GCN model\n",
        "train(gcn_model, dataloader_train, dataloader_val, epochs=50, lr=1e-3, patience=5, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VbtMkpX-u40"
      },
      "source": [
        "After training, you can make predictions using the GCN model. Save the prediction results in a `.txt` file, where each line contains one prediction for one test data point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NZNPRFS-u40"
      },
      "outputs": [],
      "source": [
        "def predict(model, dataloader, device='cuda:0'):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            #####\n",
        "            batch = batch.to(device)  # Move batch to device\n",
        "            outputs = model(batch.x, batch.edge_index, batch.batch)  # Forward pass\n",
        "            _, predicted = torch.max(outputs, dim=1)  # Get predicted class indices\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "            #####\n",
        "\n",
        "    return predictions\n",
        "\n",
        "dataset_test = GNNBenchmarkDataset(root=data_root, name='CIFAR10', split='test')\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "predictions_gcn = predict(gcn_model, dataloader_test)\n",
        "np.savetxt('predictions_gcn_cifar10.txt', predictions_gcn, fmt='%d')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h79ISD2E-u40"
      },
      "source": [
        "## Grading\n",
        "\n",
        "All the tasks will be graded by the accuracy on the test set.\n",
        "\n",
        "### Task 1 (5 points)\n",
        "- Accuracy >= 0.2: 5 points\n",
        "- Accuracy < 0.2: 0 points\n",
        "\n",
        "### Task 2 (10 points)\n",
        "- Accuracy >= 0.32: 10 points\n",
        "- Accuracy < 0.32: 0 points\n",
        "\n",
        "### Task 3 (10 points)\n",
        "- Accuracy >= 0.4: 10 points\n",
        "- Accuracy < 0.4: 0 points\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVUuPte1-u40"
      },
      "source": [
        "## Submission\n",
        "\n",
        "After completing all the tasks, you should submit the following four files to Gradescope:\n",
        "\n",
        "- `hw4_gnn.ipynb`: The notebook with all tasks completed.\n",
        "- `predictions_mlp_cluster.txt`: prediction results of the MLP model on CLUSTER dataset.\n",
        "- `predictions_gcn_cluster.txt`: prediction results of the GCN model on CLUSTER dataset.\n",
        "- `predictions_gcn_cifar10.txt`: prediction results of the GCN model on cifar10 dataset.\n",
        "\n",
        "Note that you need to submit the files individually, **DO NOT** submit a zip file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMa6VVru-u40"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
